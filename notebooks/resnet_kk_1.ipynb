{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db32b654-9409-4618-977b-71c64e36037e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m num_features \u001b[38;5;241m=\u001b[39m resnet\u001b[38;5;241m.\u001b[39mfc\u001b[38;5;241m.\u001b[39min_features\n\u001b[1;32m     71\u001b[0m resnet\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(num_features, num_classes)\n\u001b[0;32m---> 72\u001b[0m summary(resnet\u001b[38;5;241m.\u001b[39mto(device), input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m,image_size,image_size))\n\u001b[1;32m     73\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     74\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(resnet\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/miw/lib/python3.11/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(convert)\n",
      "File \u001b[0;32m~/miniconda3/envs/miw/lib/python3.11/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         module\u001b[38;5;241m.\u001b[39m_apply(fn)\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/miw/lib/python3.11/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m fn(param)\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/miw/lib/python3.11/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import os.path\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.cuda as cuda\n",
    "import torchvision.io as io\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, default_collate\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import ImageReadMode \n",
    "from torchsummary import summary\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "image_size = 64\n",
    "batch_size = 16\n",
    "num_classes = 4\n",
    "num_epochs = 8\n",
    "\n",
    "def label_images_in_directories(main_directory):\n",
    "    label_names = []\n",
    "    image_files = []\n",
    "    for directory in os.listdir(main_directory):\n",
    "        sub_directory = os.path.join(main_directory, directory)\n",
    "        if os.path.isdir(sub_directory):\n",
    "            for filename in os.listdir(sub_directory):\n",
    "                image_file = os.path.join(sub_directory, filename)\n",
    "                if os.path.isfile(image_file) and filename.endswith(\".jpg\"):\n",
    "                    label_names.append(directory)\n",
    "                    image_files.append(image_file)\n",
    "\n",
    "    image_tensors = [torchvision.io.read_image(image, mode=ImageReadMode.UNCHANGED).to(torch.float32)/255 for image in image_files]\n",
    "    nr_of_images = len(image_tensors)\n",
    "\n",
    "    return label_names, image_tensors\n",
    "\n",
    "train_dir = \"./../Project 3/apple_disease_classification/Train_clean_data\"\n",
    "label_names, image_tensors = label_images_in_directories(train_dir)\n",
    "\n",
    "test_dir = \"./../Project 3/apple_disease_classification/Test\"\n",
    "label_names, image_tensors = label_images_in_directories(test_dir)\n",
    "\n",
    "# Define the transformations before entering the neural network\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),  # Resize images to 224x224 pixels\n",
    "    transforms.ToTensor(),  # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the images\n",
    "])\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = ImageFolder(train_dir, transform=transform)\n",
    "test_dataset = ImageFolder(test_dir, transform=transform)\n",
    "\n",
    "# Create DataLoaders for managing the data batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Resnet model\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "# Change the number of output features in the last fully connected layer\n",
    "num_features = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_features, num_classes)\n",
    "summary(resnet.to(device), input_size=(3,image_size,image_size))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(resnet.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# 6. Train the model\n",
    "totalLoss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    resnet.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    losses = []\n",
    "\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = resnet(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        train_loss += loss.item() * images.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "    train_loss = train_loss / len(train_dataset)\n",
    "    train_acc = (train_correct / len(train_dataset))*100\n",
    "    totalLoss.append(sum(losses)/len(train_dataset))\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "# 7. Evaluate the model\n",
    "resnet.eval()  # Set the model to evaluation mode\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = resnet(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_samples += labels.size(0)\n",
    "        total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "test_acc = 100 * total_correct / total_samples\n",
    "print(f\"Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Save model\n",
    "torch.save(resnet.state_dict(), \"model_apple_resnet.pth\")\n",
    "\n",
    "data = {\n",
    "    \"Image Resize\": str(image_size)+\"*\"+str(image_size),\n",
    "    \"Epochs\": num_epochs,\n",
    "    \"Train Accuracy\": train_acc,\n",
    "    \"Test Accuracy\": test_acc,\n",
    "    \"Dataset use\": os.path.basename(train_dir),\n",
    "    \"Model type\": \"resnet18\"\n",
    "}\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(\"model_data.csv\"):\n",
    "    existing_data = pd.read_csv(\"model_data.csv\")\n",
    "    new_data = pd.concat([existing_data, pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "\n",
    "else:\n",
    "    new_data = pd.DataFrame(data, index=[0])\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "new_data.to_csv(\"model_data.csv\", index=False)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set(xlabel='Epoch', ylabel='Loss', title=\"Training Loss\")\n",
    "\n",
    "plt.plot(totalLoss)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ca4b93-ae08-4e96-9257-76346f81fb1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
