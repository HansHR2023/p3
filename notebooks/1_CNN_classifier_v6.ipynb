{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "758bb418-5288-4ab9-bae2-312ddef18aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 16, 224, 224]             448\n",
      "              ReLU-2         [-1, 16, 224, 224]               0\n",
      "         MaxPool2d-3         [-1, 16, 112, 112]               0\n",
      "            Conv2d-4         [-1, 32, 112, 112]           4,640\n",
      "              ReLU-5         [-1, 32, 112, 112]               0\n",
      "         MaxPool2d-6           [-1, 32, 56, 56]               0\n",
      "           Flatten-7               [-1, 100352]               0\n",
      "            Linear-8                  [-1, 128]      12,845,184\n",
      "              ReLU-9                  [-1, 128]               0\n",
      "           Linear-10                   [-1, 32]           4,128\n",
      "             ReLU-11                   [-1, 32]               0\n",
      "           Linear-12                    [-1, 4]             132\n",
      "          Dropout-13                    [-1, 4]               0\n",
      "================================================================\n",
      "Total params: 12,854,532\n",
      "Trainable params: 12,854,532\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 21.44\n",
      "Params size (MB): 49.04\n",
      "Estimated Total Size (MB): 71.05\n",
      "----------------------------------------------------------------\n",
      "Epoch 1/16: Train Loss: 1.3909, Test Loss: 1.2106, Train Accuracy: 36.91%, Val Accuracy: 55.00%, Test Accuracy: 55.00%\n",
      "Epoch 2/16: Train Loss: 1.2545, Test Loss: 1.1528, Train Accuracy: 41.36%, Val Accuracy: 48.33%, Test Accuracy: 48.33%\n",
      "Epoch 3/16: Train Loss: 1.2113, Test Loss: 1.0505, Train Accuracy: 43.72%, Val Accuracy: 58.33%, Test Accuracy: 58.33%\n",
      "Epoch 4/16: Train Loss: 1.1595, Test Loss: 1.1532, Train Accuracy: 46.60%, Val Accuracy: 46.67%, Test Accuracy: 46.67%\n",
      "Epoch 5/16: Train Loss: 1.1710, Test Loss: 0.9607, Train Accuracy: 45.55%, Val Accuracy: 57.50%, Test Accuracy: 57.50%\n",
      "Epoch 6/16: Train Loss: 1.1424, Test Loss: 1.0694, Train Accuracy: 46.34%, Val Accuracy: 59.17%, Test Accuracy: 59.17%\n",
      "Epoch 7/16: Train Loss: 1.1176, Test Loss: 0.9276, Train Accuracy: 50.52%, Val Accuracy: 65.00%, Test Accuracy: 65.00%\n",
      "Epoch 8/16: Train Loss: 1.0637, Test Loss: 0.9718, Train Accuracy: 52.88%, Val Accuracy: 59.17%, Test Accuracy: 59.17%\n",
      "Epoch 9/16: Train Loss: 1.0229, Test Loss: 1.0195, Train Accuracy: 56.81%, Val Accuracy: 57.50%, Test Accuracy: 57.50%\n",
      "Epoch 10/16: Train Loss: 0.9971, Test Loss: 1.1033, Train Accuracy: 52.88%, Val Accuracy: 50.83%, Test Accuracy: 50.83%\n",
      "Epoch 11/16: Train Loss: 1.0082, Test Loss: 1.0595, Train Accuracy: 57.33%, Val Accuracy: 55.00%, Test Accuracy: 55.00%\n",
      "Epoch 12/16: Train Loss: 0.9365, Test Loss: 1.0360, Train Accuracy: 58.38%, Val Accuracy: 58.33%, Test Accuracy: 58.33%\n",
      "Epoch 13/16: Train Loss: 0.9352, Test Loss: 1.0141, Train Accuracy: 57.33%, Val Accuracy: 60.00%, Test Accuracy: 60.00%\n",
      "Epoch 14/16: Train Loss: 0.9363, Test Loss: 1.0127, Train Accuracy: 55.24%, Val Accuracy: 57.50%, Test Accuracy: 57.50%\n",
      "Epoch 15/16: Train Loss: 0.9216, Test Loss: 1.0102, Train Accuracy: 58.38%, Val Accuracy: 58.33%, Test Accuracy: 58.33%\n",
      "Epoch 16/16: Train Loss: 0.8819, Test Loss: 1.0089, Train Accuracy: 62.04%, Val Accuracy: 58.33%, Test Accuracy: 58.33%\n",
      "2023-06-08 14:53:45\n",
      "Total duration: 00:01:00\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import random\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from datetime import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchsummary import summary\n",
    "\n",
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define constants\n",
    "image_size = 224\n",
    "batch_size = 8\n",
    "num_epochs = 16\n",
    "learning_rate = 0.001\n",
    "num_classes = 4\n",
    "dropout = 0.5\n",
    "start_time = time.time()\n",
    "\n",
    "# Function to read directories and label images\n",
    "def label_images_in_directories(main_directory):\n",
    "    label_names = []\n",
    "    image_files = []\n",
    "    for directory in os.listdir(main_directory):\n",
    "        sub_directory = os.path.join(main_directory, directory)\n",
    "        if os.path.isdir(sub_directory):\n",
    "            for filename in os.listdir(sub_directory):\n",
    "                image_file = os.path.join(sub_directory, filename)\n",
    "                if os.path.isfile(image_file) and filename.endswith(\".jpg\"):\n",
    "                    label_names.append(directory)\n",
    "                    image_files.append(image_file)\n",
    "\n",
    "    image_tensors = [torchvision.io.read_image(image, mode=ImageReadMode.UNCHANGED).to(torch.float32)/255 for image in image_files]\n",
    "    nr_of_images = len(image_tensors)\n",
    "\n",
    "    return label_names, image_tensors\n",
    "\n",
    "# Load and label the images in the training directory\n",
    "train_dir = \"../data/Train/\"\n",
    "label_names, image_tensors = label_images_in_directories(train_dir)\n",
    "\n",
    "# Load and label the images in the test directory\n",
    "test_dir = \"../data/Test\"\n",
    "label_names, image_tensors = label_images_in_directories(test_dir)\n",
    "\n",
    "# Define the transformations for train data before entering the neural network\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomCrop(size=image_size),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(30),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Define the transformations for test data before entering the neural network\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),                     # Resize images to 224x224 pixels\n",
    "    transforms.ToTensor(),                                           # Convert images to tensors\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])  # Normalize the images\n",
    "])\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Load the training and test datasets\n",
    "train_dataset = ImageFolder(train_dir, transform=transform_train)\n",
    "test_dataset = ImageFolder(test_dir, transform=transform_test)\n",
    "\n",
    "# Create DataLoaders for managing the data batches\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# # Create DataLoaders for training, validation, and test datasets\n",
    "# train_dataset = ImageFolder(train_dir, transform=transform_train)\n",
    "# train_ratio = 0.8\n",
    "# val_ratio = 0.1\n",
    "# test_ratio = 0.1\n",
    "\n",
    "# # Split the dataset into training, validation, and test sets\n",
    "# dataset_size = len(train_dataset)\n",
    "# train_size = int(train_ratio * dataset_size)\n",
    "# val_size = int(val_ratio * dataset_size)\n",
    "# test_size = dataset_size - train_size - val_size\n",
    "\n",
    "# train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
    "#     train_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# # Create DataLoaders for managing the data batches\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_classes, image_size_nn, dropout):\n",
    "        super().__init__()\n",
    "        self.CNN_Apple = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            \n",
    "            nn.Linear(32 * 56 * 56, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = self.CNN_Apple(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the model\n",
    "image_size_nn = int(image_size/4)\n",
    "model = CNNModel(num_classes, image_size_nn, dropout)\n",
    "model = model.to(device)\n",
    "\n",
    "summary(model, (3, 224, 224))\n",
    "\n",
    "# Define the loss function, optimizer, and learning rate scheduler\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=2)\n",
    "\n",
    "# Training function\n",
    "def train(model, criterion, optimizer, train_loader, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (inputs, labels) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    accuracy = 100.0 * correct / total\n",
    "    return running_loss / len(train_loader), accuracy\n",
    "\n",
    "# Test function for validation and test sets\n",
    "def test(model, criterion, loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "    accuracy = 100.0 * correct / total\n",
    "    return running_loss / len(loader), accuracy\n",
    "\n",
    "# Initialize the best loss variable with infinity\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train(model, criterion, optimizer, train_loader, device)\n",
    "    test_loss, test_accuracy = test(model, criterion, test_loader, device)\n",
    "    val_loss, val_accuracy = test(model, criterion, val_loader, device)\n",
    "\n",
    "    scheduler.step(test_loss) \n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Val Accuracy: {val_accuracy:.2f}%, Test Accuracy: {test_accuracy:.2f}%')\n",
    "    \n",
    "    # save model \n",
    "    if test_loss < best_loss:\n",
    "        best_loss = test_loss\n",
    "        torch.save(model.state_dict(), 'Team_Weight_Model.pth')\n",
    "        \n",
    "        with open(\"Team_Complete_Model.pth\", 'wb') as f:\n",
    "            torch.save(model.CNN_Apple , f)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Set a time and date\n",
    "now = datetime.now()\n",
    "timestamp = now.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(timestamp)\n",
    "\n",
    "duration = time.time() - start_time\n",
    "duration_str = time.strftime(\"%H:%M:%S\", time.gmtime(duration))\n",
    "print(f\"Total duration: {duration_str}\")\n",
    "        \n",
    "data = {\n",
    "    \"Timestamp\": timestamp,\n",
    "    \"Duration\": duration_str,\n",
    "    \"Model type\": \"Team model\",\n",
    "    \"Dataset use\": os.path.basename(train_dir),\n",
    "    \"Image Resize\": str(image_size)+\"*\"+str(image_size),\n",
    "    \"Epochs\": num_epochs,\n",
    "    \"Learning rate\": learning_rate,\n",
    "    \"Batch size\": batch_size,\n",
    "    \"Train Accuracy\": f\"{train_accuracy:.2f}\",\n",
    "    \"Validation accuracy\": f\"{val_accuracy:.2f}\",\n",
    "    \"Test Accuracy\": f\"{test_accuracy:.2f}\",\n",
    "}\n",
    "# Check if the CSV file already exists\n",
    "if os.path.isfile(\"model_data.csv\"):\n",
    "    existing_data = pd.read_csv(\"model_data.csv\")\n",
    "    new_data = pd.concat([existing_data, pd.DataFrame(data, index=[0])], ignore_index=True)\n",
    "else:\n",
    "    new_data = pd.DataFrame(data, index=[0])\n",
    "\n",
    "# Save the updated DataFrame to CSV\n",
    "new_data.to_csv(\"model_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be58865-5f42-44a7-8c83-3bb32804bb1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
